### BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding(2019)

> 论文链接：https://arxiv.org/pdf/1810.04805.pdf
>
> 代码地址：https://github.com/google-research/bert

对题目的理解：BERT是一个深的、双向的transformer，是用于做预训练的，针对的是一般的NLP任务。

### Abstract

本文引入了BERT模型(Transformer的双向编码表示)，BERT使用没有标签的文本预训练深的、双向的表示，联合**左右**的上下文信息。最终，预训练好的BERT模型可以在只使用一个额外输出层的情况下进行微调，就可以得到一个不错的结果(在许多NLP任务上，包括问答、语言推理)，而不需要对特定任务做架构上的改变。以上两句话，分别是讲它和ELMO以及GPT的区别(GPT考虑的是单向的，用左边的信息去预测未来的信息，BERT用左边和右边的信息，所以是双向的。而ELMO用的是基于RNN的架构，而BERT用的是Transformer，所以ELMO在用到一些下游任务的时候，需要对架构做微调，但BERT只需要改最上层就行了)。

摘要的第二段有一个十分推崇的写法：当你说你的结果比别人好的时候，要讲清楚两个东西，第一个是你的绝对精度是多少，第二个是和别人比，你的相对好处(提升)是多少。这样一来，就算读者不是很了解数据集，也能知道你的结果是十分显著的。

### Introduction

在将预训练语言模型应用到下游任务时，有两种策略：基于特征的和基于微调的。前者的代表作是EMLO，对每一个下游的任务，构造一个和这个任务相关的**双向**神经网络，但架构比较老(EMLO用的是RNN架构)，预训练好的表示会作为一个额外的特征和输入一起进入模型中；后者的代表作是GPT，用的架构比较新(transformer)，但只能处理单向信息。把预训练好的模型放在下游任务的时候不需要改变太多，需要改一点就行了。模型预训练好的参数会在下游的数据上进行微调。以上两种策略在预训练阶段都是使用相同的目标函数，使用单向的语言模型去学习通用的语言表示。

现有的方法在做预训练表征的时候有局限性(尤其是基于微调的策略)，主要的限制是标准的语言模型是单向的，这使得在选择架构时有局限性。比如在GPT中，使用的是从左到右的架构，这不是很好。

本文在改进fine-tuning based策略的基础上提出了BERT模型，用来缓解之前提出的语言模型是单向的限制，方法是使用一个带掩码的语言模型(MLM)。MLM每一次会随机地掩盖住输入中某些token，目标函数是预测那些被盖住的token。和标准的语言模型从左看到右的方式不同，MLM允许看到左右的信息(在做完形填空时，不能只关注空的左边，还要关注空的右边)，因此训练了一个双向的transformer模型。在MLM之外，还训练了一个别的任务(next sentense prediction)，核心思想是：给定两个句子，判断出这两个句子在原文中是否是相邻的，还是随机采样的两个句子放在一起，这样就能让模型能去学习句子层面的信息。

#### 贡献

- 展示了双向信息的重要性。GPT只用了单向信息。
- 假如有一个比较好的预训练模型的话，就不需要为特定任务做特定改动了。BERT是第一个基于微调的模型。
- 代码和模型全部开源。

### Conclusion

我们的主要贡献是将单向架构进一步推广到深层双向体系结构，从而允许相同的预训练模型成功地处理广泛的NLP任务。EMLO用了双向的信息，但是架构比较老，用的是RNN架构；另一个是GPT，用的架构比较新(transformer)，但只能处理单向信息。本文把EMLO双向的想法和GPT使用transformer的想法合起来，就成为了BERT。具体的改动：在做语言模型的时候不是在预测未来，而是变成完形填空。简言之，本文做的工作就是A + B。

### Related Work

#### Unsupervised Feature-based Approaches

讲的其实就是EMLO、之前和之后的工作。

#### Unsupervised Fine-tuning Approaches

讲的其实就是GPT、之前和之后的工作。

#### 有监督学习的迁移学习

BERT和其之后的一系列工作证明了在**NLP领域**没有标签的**大规模**数据集训练出来的模型效果比在有标签的**相对小一点**的数据集训练的模型更好。

### BERT

BERT架构的实现有两个步骤：预训练和微调。

在预训练中，模型是在无标签的数据中训练的；在微调中，BERT模型首先使用预训练的参数进行初始化，所有的参数都会使用下游任务中带有标签的数据进行微调。每一个下游任务都有不同的微调模型，即使在初始化的时候使用的是相同的预训练参数。

![](https://cdn.jsdelivr.net/gh/prannt99/blog/img/29.png)

在预训练时，输入是没有标号的句子对(sentence pair)，在无标签的数据中训练出BERT模型(参数训练好)。对于每一个下游的任务，创建一个**相同的**BERT模型，对于每一个下游任务，都会有自己的代标签的数据，然后对BERT继续训练，得到不同任务的BERT版本。

### Architecture

BERT模型是一个多层的、双向的Transformer编码器结构。

本文调了三个参数，L(Transformer块的个数)，H(隐藏层尺寸)和A(self-attention中多头的头的个数)。本文训练了两个模型：BERT<sub>BASE</sub>和BERT<sub>LARGE</sub>。

#### 回顾

在Transformer中，可学习参数主要来自于两个地方：嵌入层和Transformer Block。

**嵌入层：**

